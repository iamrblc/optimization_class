{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO OPTIMIZATION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Tradeoffs: Speed vs Memory vs Readability\n",
    "\n",
    "In data-heavy computing (hello bioinformatics) you often face tradeoffs between: speed, memory usage and code readibility. \n",
    "\n",
    "Pick oneâ€¦ or maybe twoâ€¦ \n",
    "\n",
    "### Python list vs NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python list total size: 3515.68 KB\n",
      "NumPy array total size: 1562.61 KB\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "import numpy as np\n",
    "\n",
    "py_list = list(range(100000))\n",
    "np_array = np.array(range(100000))\n",
    "\n",
    "list_total = getsizeof(py_list) + sum(getsizeof(x) for x in py_list)\n",
    "np_total = getsizeof(np_array) + np_array.nbytes\n",
    "\n",
    "print(f\"Python list total size: {list_total / 1024:.2f} KB\")\n",
    "print(f\"NumPy array total size: {np_total / 1024:.2f} KB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage: 80.00 MB\n",
      "Peak memory usage: 80.01 MB\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "CAREFUL! This code will allocate a huge list in memory\n",
    "and it might crash your system.\n",
    "If you run it, make sure you have enough memory available.\n",
    "'''\n",
    "'''\n",
    "import tracemalloc\n",
    "tracemalloc.start()\n",
    "\n",
    "# Allocate something big\n",
    "huge_list = [0] * 10_000_000\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "print(f\"Current memory usage: {current / 10**6:.2f} MB\")\n",
    "print(f\"Peak memory usage: {peak / 10**6:.2f} MB\")\n",
    "tracemalloc.stop()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare data structures\n",
    "\n",
    "# Python list of dictionaries\n",
    "py_dicts = [\n",
    "    {\"chrom\": \"chr1\", \"start\": 100, \"end\": 200},\n",
    "    {\"chrom\": \"chr2\", \"start\": 300, \"end\": 400},\n",
    "]\n",
    "# List of tuples\n",
    "py_tuples = [\n",
    "    (\"chr1\", 100, 200),\n",
    "    (\"chr2\", 300, 400),\n",
    "]\n",
    "# NumPy array\n",
    "import numpy as np\n",
    "np_array = np.array([\n",
    "    [\"chr1\", 100, 200],\n",
    "    [\"chr2\", 300, 400],\n",
    "])\n",
    "# Pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"chrom\": [\"chr1\", \"chr2\"],\n",
    "    \"start\": [100, 300],\n",
    "    \"end\": [200, 400],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python list of dicts: 978 bytes\n",
      "Python list of tuples: 418 bytes\n",
      "NumPy array (object dtype): 1136 bytes\n",
      "Pandas DataFrame: 286 bytes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Size of Python list of dicts\n",
    "dicts_size = sys.getsizeof(py_dicts) + sum(sys.getsizeof(d) + sum(sys.getsizeof(k) + sys.getsizeof(v) for k, v in d.items()) for d in py_dicts)\n",
    "\n",
    "# Size of Python list of tuples\n",
    "tuples_size = sys.getsizeof(py_tuples) + sum(sys.getsizeof(t) + sum(sys.getsizeof(i) for i in t) for t in py_tuples)\n",
    "\n",
    "# Size of NumPy array\n",
    "np_total_size = sys.getsizeof(np_array) + np_array.nbytes\n",
    "\n",
    "# Size of Pandas DataFrame\n",
    "df_size = df.memory_usage(deep=True).sum()\n",
    "\n",
    "print(f\"Python list of dicts: {dicts_size} bytes\")\n",
    "print(f\"Python list of tuples: {tuples_size} bytes\")\n",
    "print(f\"NumPy array (object dtype): {np_total_size} bytes\")\n",
    "print(f\"Pandas DataFrame: {df_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hang on, why is NumPy array so large? It's supposed to be the most effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<U21')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U21 means that it's a unicode string where each string element is allocated 21 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python list of dicts: 978 bytes\n",
      "Python list of tuples: 418 bytes\n",
      "NumPy array (object dtype): 1136 bytes\n",
      "NumPy array (optimized): 208 bytes\n",
      "Pandas DataFrame: 286 bytes\n"
     ]
    }
   ],
   "source": [
    "# Let's improve numpy array size\n",
    "dtype = [(\"chrom\", \"U4\"), (\"start\", \"i4\"), (\"end\", \"i4\")]\n",
    "np_array_optimized = np.array([\n",
    "    (\"chr1\", 100, 200),\n",
    "    (\"chr2\", 300, 400),\n",
    "], dtype=dtype)\n",
    "\n",
    "np_total_size_optimized = sys.getsizeof(np_array_optimized) + np_array_optimized.nbytes\n",
    "print(f\"Python list of dicts: {dicts_size} bytes\")\n",
    "print(f\"Python list of tuples: {tuples_size} bytes\")\n",
    "print(f\"NumPy array (object dtype): {np_total_size} bytes\")\n",
    "print(f\"NumPy array (optimized): {np_total_size_optimized} bytes\")\n",
    "print(f\"Pandas DataFrame: {df_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the right language\n",
    "Let's see a problem that takes time, adding up the first billion numbers. \n",
    "\n",
    "In Python you can do it by using a loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "N = 1000_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to add them up \"manually\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop total: 499999999500000000\n",
      "Loop time: 46.79621720314026 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "total_loop = 0\n",
    "for i in range(N):\n",
    "    total_loop += i\n",
    "\n",
    "print(f\"Loop total: {total_loop}\")\n",
    "print(f\"Loop time: {time.time() - start} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it took some time even on a strong machine. Let's try the same with Python's native `sum()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum total: 499999999500000000\n",
      "Sum time: 7.801518201828003 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "total_sum = sum(range(N))\n",
    "print(f\"Sum total: {total_sum}\")\n",
    "print(f\"Sum time: {time.time() - start} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was much faster. When python runs this function, there is a low level language running in the background, similar to this `C++` script:\n",
    "\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <chrono>\n",
    "\n",
    "int main() {\n",
    "auto start = std::chrono::high_resolution_clock::now();\n",
    "long long total = 0;\n",
    "for (int i = 0; i < 1000000000; ++i) total += i;\n",
    "std::cout << total << std::endl;\n",
    "auto end = std::chrono::high_resolution_clock::now();\n",
    "std::chrono::duration<double> diff = end - start;\n",
    "std::cout << \"Time: \" << diff.count() << \" s\\n\";\n",
    "return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### Interpreted vs Compiled languages\n",
    "\n",
    "A **compiled language** (like C++) translates the entire code into machine instructions **before** it runs. So when you execute it, it's already in the fastest form your computer understands.\n",
    "\n",
    "An **interpreted language** (like Python) reads and executes code **line by line**, while the program is running. This makes it more flexible and easy to write, but slower because it's doing more work at runtime.\n",
    "\n",
    "Thatâ€™s why Python is often used for prototyping. It may be too slow for really big tasks, but it's easy to write and debug. Once the code is running as intended, you can \"translate\" it to a low level language. \n",
    "\n",
    "### JIT - Just In Time Compilation\n",
    "There are languages that were developed specifically for scientific computing, such as Matlab or Julia. The latter is especially known for being \"easy as Python, fast as C\". \n",
    "\n",
    "Julia looks like an interpreted language (just like Python), but in reality it uses **just in time compliation (JIT)**. It doesn't compile the entire code before it runs (like C), but compiles parts of it as needed, on the go. \n",
    "\n",
    "While Juila or Matlab were designed to be able to do this there are also Python packages (such as `numba`) that enable JIT.\n",
    "\n",
    "```julia\n",
    "@time total = sum(0:999_999_999)\n",
    "println(total)\n",
    "```\n",
    "(Note that Julia uses 1-indexing instead of 0-indexing, similar to R.)\n",
    "\n",
    "You may notice that the runtime was extremely low. But how can it be even faster than a language close to machine code?\n",
    "\n",
    "In scientific computing languages like Julia, many mathematical patterns are recognized and mapped to hardcoded optimizations. In this case, Julia detects that ranges like `(n:m)` are a special case for `sum()` and uses a fast, pre-optimized formula instead of looping (here a `UnitRange` method).\n",
    "\n",
    "Conceptually, this is similar to the trick the little Gauss allegedly used to sum numbers quickly:\n",
    "\n",
    "$\\sum_{i=1}^{n} i = \\frac{n(n+1)}{2}$\n",
    "\n",
    "Take home message: using your brain can speed up your code. ðŸ™ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum total: 499999999500000000\n",
      "Sum time: 0.00015211105346679688 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "n = N - 1\n",
    "total_sum = n * (1 + n) // 2\n",
    "print(f\"Sum total: {total_sum}\")\n",
    "print(f\"Sum time: {time.time() - start} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See?\n",
    "\n",
    "### Self check questions\n",
    "When is it worth to use a compiled languge?<br>\n",
    "Why is Python widely used in scientific tasks despite its slow performance?<br>\n",
    "What makes Julia \"as easy as Python and as fast as C\"?<br>\n",
    "What happens when you sum random numbers instead of a range?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add up 100 thousand random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49998298.34697442\n",
      "5.654012203216553\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from random import random, seed\n",
    "seed(42)\n",
    "\n",
    "start = time()\n",
    "print(sum([random() for _ in range(100_000_000)]))\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical CPUs (threads): 8\n",
      "Physical cores: 8\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(\"Logical CPUs (threads):\", psutil.cpu_count(logical=True))\n",
    "print(\"Physical cores:\", psutil.cpu_count(logical=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, multiprocessing and Jupyter notebook don't go well hand-in-hand, so consult the README.md for this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random FASTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FASTA file generated at ../data/sequences.fasta\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_fasta(file_path, num_sequences=1000, seq_length=1000):\n",
    "    bases = ['A', 'C', 'G', 'T']\n",
    "    with open(file_path, 'w') as f:\n",
    "        for i in range(num_sequences):\n",
    "            seq_id = f\">seq{i+1}\\n\"\n",
    "            sequence = ''.join(random.choices(bases, k=seq_length)) + '\\n'\n",
    "            f.write(seq_id)\n",
    "            f.write(sequence)\n",
    "\n",
    "output_path = '../data/sequences.fasta'\n",
    "generate_fasta(output_path, num_sequences=10, seq_length=1000)\n",
    "print(f\"FASTA file generated at {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rblc/ELTE/optimization_class/.venv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
